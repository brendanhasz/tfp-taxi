{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa309aa09d7f2b549d30c5be1800b068ae1fec04"
      },
      "cell_type": "code",
      "source": "#!pip install -q tfp-nightly",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport time\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf\n#import tensorflow_probability as tfp\n#from tensorflow_probability import edward2 as ed\n#tfd = tfp.distributions\n\n\nsns.set()\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n%config InlineBackend.figure_format = 'svg'\nnp.random.seed(111)\ntf.set_random_seed(111)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27acd520a99ba3627767ee97dbc8fffd1593d167"
      },
      "cell_type": "markdown",
      "source": "## Data\n\nTODO: intro"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv', \n                    usecols=np.arange(1, 7),\n                    nrows=200000)\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'],\n                                          format='%Y-%m-%d %H:%M:%S %Z')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "309cc9a73107f7ca079d52a76efc1c020b0996ba"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf1a366ec98e01e78f2ab6c84d6aab7cad34ea9c"
      },
      "cell_type": "markdown",
      "source": "Let's take a look at how many null values there are in each column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29cb1472a732fa8544982ff2aef15908c737f3be"
      },
      "cell_type": "code",
      "source": "print('Column\\tPercent Null')\nfor col in train:\n    print(col, 100*train[col].isnull().sum()/train.shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0cd56dfe776e4771f0a61baee5567329764d3984"
      },
      "cell_type": "markdown",
      "source": "There are some null values, but a negligible amount of them, so we'll simply remove rows with null values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75c009b90d8cc7067bc40acd4bd74b47832e268a"
      },
      "cell_type": "code",
      "source": "train.dropna(inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fedc1c6e113aab36ec5247bc9e1c175845d97748"
      },
      "cell_type": "markdown",
      "source": "Have to extract time of day, time of week, time of year, and year, and then drop the original datetime column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee1a540ce6a05f716b6db84bbca9e740b814bd7f"
      },
      "cell_type": "code",
      "source": "train['min_of_day'] = (60*train['pickup_datetime'].dt.hour + \n                       train['pickup_datetime'].dt.minute)\ntrain['day_of_week'] = train['pickup_datetime'].dt.dayofweek\ntrain['day_of_year'] = train['pickup_datetime'].dt.dayofyear\ntrain['year'] = train['pickup_datetime'].dt.year\ntrain.drop('pickup_datetime', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ebab7117bf8449334185941a5ec024033e08639"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a0f025170431d4ab183c8913922924e44174be5"
      },
      "cell_type": "markdown",
      "source": "Let's check the fare amount distributions:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5a656fa499d0c8c54f4a247e48643e3a978d44e"
      },
      "cell_type": "code",
      "source": "# Plot distribution of fares\nsns.distplot(train['fare_amount'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc0e468a9c0d56e8645c004f43e280e9035f9e71"
      },
      "cell_type": "markdown",
      "source": "It looks like there might be some negative values (which doesn't make any sense, of course!). Let's zoom in on the area around 0."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44538620782a1884fe96749ced6089d3a81e9583"
      },
      "cell_type": "code",
      "source": "# Plot distribution of fares around 0\nplt.hist(train['fare_amount'], \n         bins=np.arange(-50, 10), log=True)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9e7e20df196f428b46a414b6fa195098bbf319c5"
      },
      "cell_type": "markdown",
      "source": "Let's remove the datapoints with fares which are suspiciously low, and also rides with suspiciously high fares."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c7fa0cda7d21a8c6380f7e561085ba96676b91b"
      },
      "cell_type": "code",
      "source": "# Function to remove rows outside range\ndef clip(df, a, b, col):\n    for c in col:\n        df = df[(df[c]>a) & (df[c]<b)]\n    return df\n\n# Remove rows with outlier fare values\ntrain = clip(train, 1, 200, ['fare_amount'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3e476dfd9232e7ad42d54dcc7b550f10383e575c"
      },
      "cell_type": "markdown",
      "source": "Finally, let's check the locations of the pickups and dropoffs."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f55034c1c499ffafeac2db754f8ab38a600f95f2"
      },
      "cell_type": "code",
      "source": "# Plot distribution of pickup longitudes\nfig, ax = plt.subplots(2, 2)\nnyc_lon = -74\nnyc_lat = 40.7\nax[0,0].axvline(nyc_lon, linestyle='--', color='k')\nax[0,0].hist(train['pickup_longitude'], bins=50, log=True)\nax[0,0].set_ylabel('Pickup')\nax[1,0].axvline(nyc_lon, linestyle='--', color='k')\nax[1,0].hist(train['dropoff_longitude'], bins=50, log=True)\nax[1,0].set_xlabel('Longitude')\nax[1,0].set_ylabel('Dropoff')\nax[0,1].axvline(nyc_lat, linestyle='--', color='k')\nax[0,1].hist(train['pickup_latitude'], bins=50, log=True)\nax[1,1].axvline(nyc_lat, linestyle='--', color='k')\nax[1,1].hist(train['dropoff_latitude'], bins=50, log=True)\nax[1,1].set_xlabel('Latitude')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28c50beadeb9be1e3e2d33a93ffd2f666f29699b"
      },
      "cell_type": "markdown",
      "source": "There are some outliers, especially at 0.  Let's remove rows with geographical locations outside a reasonable range (near the greater NYC metropolitan area)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ce1a1e7e5bf7185bf1bd478c11c54118db1664e"
      },
      "cell_type": "code",
      "source": "# Remove geographical outliers\ntrain = clip(train,  -75, -72.5,\n             ['pickup_longitude', 'dropoff_longitude'])\ntrain = clip(train, 40, 41.5,\n             ['pickup_latitude', 'dropoff_latitude'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dda1c02b5f2088a204d296e2fba3cf43fc91eb55"
      },
      "cell_type": "markdown",
      "source": "And now we have only values which are near NYC:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4730f6d055735ca954866abcc62590d1ed9f3aa1"
      },
      "cell_type": "code",
      "source": "# Plot distribution of pickup longitudes\nfig, ax = plt.subplots(2, 2)\nnyc_lon = -74\nnyc_lat = 40.7\nax[0,0].axvline(nyc_lon, linestyle='--', color='k')\nax[0,0].hist(train['pickup_longitude'], bins=50, log=True)\nax[0,0].set_ylabel('Pickup')\nax[1,0].axvline(nyc_lon, linestyle='--', color='k')\nax[1,0].hist(train['dropoff_longitude'], bins=50, log=True)\nax[1,0].set_xlabel('Longitude')\nax[1,0].set_ylabel('Dropoff')\nax[0,1].axvline(nyc_lat, linestyle='--', color='k')\nax[0,1].hist(train['pickup_latitude'], bins=50, log=True)\nax[1,1].axvline(nyc_lat, linestyle='--', color='k')\nax[1,1].hist(train['dropoff_latitude'], bins=50, log=True)\nax[1,1].set_xlabel('Latitude')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "149e5979f4be5aba7f4f19b20cdb364b054624e7"
      },
      "cell_type": "markdown",
      "source": "## Baseline Model\n\nxgboost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb1c07c246114f3f38e11032b18b7855e1b6e31b"
      },
      "cell_type": "code",
      "source": "# Separate in- from dependent variables\nx_taxi = train.drop('fare_amount', axis=1)\ny_taxi = train['fare_amount']\n\n# Make Mean Absolute Error scorer\nmae_scorer = make_scorer(mean_absolute_error)\n\n# Function to print cross-validated mean abs deviation\ndef cv_mae(regressor, x, y, cv=3, scorer=mae_scorer):\n    scores = cross_val_score(regressor, \n                             x, y, cv=cv,\n                             scoring=scorer)\n    print('MAE:', scores.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8d8e507c776878815242f0afc8793c86de9fed2"
      },
      "cell_type": "markdown",
      "source": "How well do we do if we just predict the mean?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "619844e8d9fa1f42197c3fe3827f5767f335a3bd"
      },
      "cell_type": "code",
      "source": "# MAE from predicting just the mean\ncv_mae(DummyRegressor(), x_taxi, y_taxi)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "11212cfbdd62568aa72cd1db87c011b257179c7a"
      },
      "cell_type": "markdown",
      "source": "And if we just use the distance of the trip as a predictor?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6922b190adf7815fe0114d02eb84c6ae5b3bce05"
      },
      "cell_type": "code",
      "source": "# Distance between pickup and dropoff locations\ndist = np.sqrt(\n    np.power(train['pickup_longitude'] -\n             train['dropoff_longitude'], 2) + \n    np.power(train['pickup_latitude'] - \n             train['dropoff_latitude'], 2))\n\n# MAE from using just distance as predictor\ncv_mae(IsotonicRegression(out_of_bounds='clip'), \n       dist, y_taxi)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9aec5c8c778c81f4439e32ae2b1fb4cdff780a89"
      },
      "cell_type": "markdown",
      "source": "And we can do a little better if we use gradient boosted decision trees."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27f2291ca073d4f840fc4a2aa8e8a752c4267a60"
      },
      "cell_type": "code",
      "source": "# Cross-validated MAE w/ CatBoost\ncv_mae(CatBoostRegressor(logging_level='Silent'), \n       x_taxi, y_taxi)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cfed1736b9378f33aaab910ff121af994f0dca2"
      },
      "cell_type": "markdown",
      "source": "## Uncertainty under Heteroskedasticity\n\nWhat if we now want to predict our uncertainty as to our estimate?\n\nBut need to look out for heteroskedasticity!  Explain heteroskedasticity"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9180b95993ad50542bad5fccd24e493ee70040d6"
      },
      "cell_type": "code",
      "source": "# A function to generate heteroskedastic data\ndef h_func(x):\n    return x + np.exp(0.5*x)*np.random.randn(x.shape[0],1)\n\n# Generate heteroskedastic data\nN = 1000\nxx = np.atleast_2d(np.random.randn(N)).T\nyy = h_func(xx).ravel()\n\n# Generate validation data\nxx_val = np.atleast_2d(np.linspace(-3, 3, N)).T\nyy_val = h_func(xx_val).ravel()\n\n# Plot data we generated\nplt.plot(xx, yy, '.')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "601eb9f353d924b5eb5c321743c61f675270886e"
      },
      "cell_type": "markdown",
      "source": "Is the true data heteroskedastic?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3607050ec74c50c59de23d4326a2b123f32bbe0b"
      },
      "cell_type": "code",
      "source": "# Plot distance vs fare\nsns.jointplot('Distance', 'Fare Amount',\n              pd.DataFrame({'Distance':dist, \n                            'Fare Amount':y_taxi}), \n              kind=\"hex\",\n              xlim=(0, 0.4), ylim=(0, 100),\n              joint_kws=dict(gridsize=70, bins='log'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4822098717d1a1f86e80b939473101a03031c7d3"
      },
      "cell_type": "markdown",
      "source": "Indeed it looks like the true data is heteroskedastic.  And keep in mind this is only the *distance*, whereas the full dataset has pickup/dropoff location, time of day, time of year, etc.  The y-values could be heteroskedastic as a function of those predictors as well!\n\nHow can we account for this uncertainty, especially when it is varying with our independent variables?  One way which can be used with gradient boosted decision trees is to do a quantile regression.\n\nExplain quantile loss, penalizes y_pred>y_true more than y_true>y_pred when quantile<0.5 and vice-versa.  Eqs etc"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "171aa9985b9f8151078657e16031af2641126f07"
      },
      "cell_type": "code",
      "source": "# Gradient boosted tree regressors w/ different quantile losses\ngbrL = CatBoostRegressor(loss_function='Quantile:alpha=0.025', logging_level='Silent')\ngbr = CatBoostRegressor(loss_function='Quantile:alpha=0.5', logging_level='Silent')\ngbrH = CatBoostRegressor(loss_function='Quantile:alpha=0.975', logging_level='Silent')\n\n# Using scikit-learn's gradient boosted decision trees\n#from sklearn.ensemble import GradientBoostingRegressor\n#gbrL = GradientBoostingRegressor(loss='quantile', alpha=0.025)\n#gbr = GradientBoostingRegressor(loss='quantile', alpha=0.5)\n#gbrH = GradientBoostingRegressor(loss='quantile', alpha=0.975)\n\n# Fit to data\ngbrL.fit(xx, yy)\ngbr.fit(xx, yy)\ngbrH.fit(xx, yy)\n\n# Predict on validation data\ny_predL = gbrL.predict(xx_val)\ny_pred = gbr.predict(xx_val)\ny_predH = gbrH.predict(xx_val)\n\n# Plot predictions over points\nplt.figure()\nplt.plot(xx_val, yy_val, '.',\n         label='Validation data')\nplt.fill_between(xx_val.ravel(), y_predL, y_predH,\n                 alpha=0.3, facecolor=colors[2],\n                 label='95% confidence interval')\nplt.plot(xx_val, y_pred, 'k', label='Predicted median')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5f29e25624112b35f07178289f21a3152502823"
      },
      "cell_type": "markdown",
      "source": "To see how well-calibrated the model is, we can check the coverage of the 95% confidence interval (the percentage of y values from the validation dataset falling within our 95% predictive interval).  If the model is well-calibrated, the coverage will be near 95%."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d4d9185136bff9f6fa654a6eb9c010468a0c553"
      },
      "cell_type": "code",
      "source": "# Function to compute coverage of predictive interval\ndef coverage(y, yL, yH):\n     return (100 / yL.shape[0] *\n             ((y>yL)&(y<yH)).sum())\n    \n# Compute coverage of the 95% interval\nprint('Coverage of 95%% predictive interval: %0.1f%%'\n      % coverage(yy_val, y_predL, y_predH))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "546cc8de315109c04712905716f4201f055e46a4"
      },
      "cell_type": "markdown",
      "source": "Hmm. OK but not great.  How does it look on the real data?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6aee300942aefccaa77f2467264105efc97e08a7"
      },
      "cell_type": "code",
      "source": "# Compute 2.5% and 97.5% predictive intervals\ny_predL = cross_val_predict(gbrL, x_taxi, y_taxi)\ny_predH = cross_val_predict(gbrH, x_taxi, y_taxi)\n\n# Compute coverage of the 95% interval\nprint('Coverage of 95%% predictive interval: %0.1f%%'\n      % coverage(y_taxi, y_predL, y_predH))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "450447c94dc8530574bfaeef6242f2397be676a3"
      },
      "cell_type": "markdown",
      "source": "Eek.  Not so great at all :(\n\nWe could calibrate our model, or we could use a model which is more explicitly built to capture uncertainty accurately!"
    },
    {
      "metadata": {
        "_uuid": "08b52b51b0e0360e441eaedb6483993dddfd472e"
      },
      "cell_type": "markdown",
      "source": "## Dual-module Bayesian Neural Network\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}