{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFP_Taxi",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "KdujiISZPEl0"
      },
      "cell_type": "markdown",
      "source": [
        "# Dual-module Bayesian Neural Network\n",
        "\n",
        "TODO: intro\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JvkYeJkNO3th",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tf-nightly\n",
        "!pip install -q tfp-nightly\n",
        "!pip install -q catboost\n",
        "!pip install -q --pre vaex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LY9NbmFHO6-W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import vaex\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from catboost import CatBoostRegressor\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Settings\n",
        "sns.set()\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "np.random.seed(111)\n",
        "tf.set_random_seed(111)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQSVyBp-minb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "TODO: explain, mention more eda in previous post"
      ]
    },
    {
      "metadata": {
        "id": "41GRdFiGOxC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvpoZQeKO_Nc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Columns to load\n",
        "dtypes = {'fare_amount':      'float32',\n",
        "          'pickup_datetime':  'str', \n",
        "          'pickup_longitude': 'float32',\n",
        "          'pickup_latitude':  'float32',\n",
        "          'dropoff_longitude':'float32',\n",
        "          'dropoff_latitude': 'float32'}\n",
        "\n",
        "# Load data\n",
        "#train = pd.read_csv('train.csv', \n",
        "train = pd.read_csv('/content/gdrive/My Drive/DataScience/Data/train.zip', \n",
        "                    usecols=dtypes.keys(), \n",
        "                    dtype=dtypes,\n",
        "                    nrows=1000000)\n",
        "\n",
        "# Convert pickup time column to datetime\n",
        "train['pickup_datetime'] = train['pickup_datetime'].str.slice(0, 16)\n",
        "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], \n",
        "                                          utc=True, format='%Y-%m-%d %H:%M')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdqqUEl2QV-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drop rows with empty values\n",
        "train.dropna(inplace=True)\n",
        "\n",
        "# Extract useful time information\n",
        "train['min_of_day'] = (60*train['pickup_datetime'].dt.hour + \n",
        "                       train['pickup_datetime'].dt.minute).astype('int32')\n",
        "train['day_of_week'] = train['pickup_datetime'].dt.dayofweek.astype('int32')\n",
        "train['day_of_year'] = train['pickup_datetime'].dt.dayofyear.astype('int32')\n",
        "train['year'] = train['pickup_datetime'].dt.year.astype('int32')\n",
        "\n",
        "# Remove original datetime column\n",
        "train.drop('pickup_datetime', axis=1, inplace=True)\n",
        "\n",
        "# Function to remove rows outside range\n",
        "def clip(df, a, b, col):\n",
        "    for c in col:\n",
        "        df = df[(df[c]>a) & (df[c]<b)]\n",
        "    return df\n",
        "\n",
        "# Remove outliers\n",
        "train = clip(train, 1, 200, ['fare_amount'])\n",
        "train = clip(train,  -75, -72.5,\n",
        "             ['pickup_longitude', 'dropoff_longitude'])\n",
        "train = clip(train, 40, 41.5,\n",
        "             ['pickup_latitude', 'dropoff_latitude'])\n",
        "\n",
        "# Transform target column\n",
        "train['fare_amount'] = np.log(np.log10(train['fare_amount']))\n",
        "\n",
        "# Normalize data\n",
        "train = (train - train.mean()) / train.std()\n",
        "\n",
        "# Separate in- from dependent variables\n",
        "x_taxi = train.drop('fare_amount', axis=1)\n",
        "y_taxi = train['fare_amount']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVSvi4I8moAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Models"
      ]
    },
    {
      "metadata": {
        "id": "Nt7Yirg4cTh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make Mean Absolute Error scorer\n",
        "mae_scorer = make_scorer(mean_absolute_error)\n",
        "\n",
        "# Function to print cross-validated mean abs deviation\n",
        "def cv_mae(regressor, x, y, cv=3, scorer=mae_scorer):\n",
        "    scores = cross_val_score(regressor, \n",
        "                             x, y, cv=cv,\n",
        "                             scoring=scorer)\n",
        "    print('MAE:', scores.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGGXJDeNcZyX",
        "colab_type": "code",
        "outputId": "7b1f79a7-ff2b-4f14-feb6-9fbdc3790ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE from predicting just the mean\n",
        "cv_mae(DummyRegressor(), x_taxi, y_taxi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 0.7930104533831278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b7u2_TRXcXgG",
        "colab_type": "code",
        "outputId": "2b8861c9-6251-43a9-b3d1-1ff8af124c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Distance between pickup and dropoff locations\n",
        "dist = np.sqrt(\n",
        "    np.power(x_taxi['pickup_longitude'] -\n",
        "             x_taxi['dropoff_longitude'], 2) + \n",
        "    np.power(x_taxi['pickup_latitude'] - \n",
        "             x_taxi['dropoff_latitude'], 2))\n",
        "\n",
        "# MAE from using just distance as predictor\n",
        "cv_mae(IsotonicRegression(out_of_bounds='clip'), \n",
        "       dist, y_taxi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 0.36685405935010573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XCdOGxErmqtz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vanilla Neural Network\n",
        "\n",
        "TODO: talk about just prediction w/ nnet w/ MAE"
      ]
    },
    {
      "metadata": {
        "id": "9IaDDO71PGsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxLCv2JYLr3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run simple keras model to get MAE\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(x_taxi.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation=None)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2zkyz9dyPDY5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compile the model using MAE\n",
        "model.compile('adam',\n",
        "              loss='mean_absolute_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKN-hzn4SShl",
        "colab_type": "code",
        "outputId": "49f96c92-cd1f-4ff2-f202-16bda6be051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# Stop training when we start to overfit\n",
        "callbacks = [EarlyStopping(restore_best_weights=True,\n",
        "                           patience=2)]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_taxi, y_taxi,\n",
        "          batch_size=1000,  epochs=10,\n",
        "          validation_split=0.5,\n",
        "          callbacks=callbacks,  verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 489552 samples, validate on 489552 samples\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.5191 - val_loss: 0.3526\n",
            "Epoch 2/10\n",
            " - 5s - loss: 0.3952 - val_loss: 0.3324\n",
            "Epoch 3/10\n",
            " - 5s - loss: 0.3795 - val_loss: 0.3238\n",
            "Epoch 4/10\n",
            " - 5s - loss: 0.3735 - val_loss: 0.3204\n",
            "Epoch 5/10\n",
            " - 5s - loss: 0.3710 - val_loss: 0.3211\n",
            "Epoch 6/10\n",
            " - 5s - loss: 0.3686 - val_loss: 0.3136\n",
            "Epoch 7/10\n",
            " - 5s - loss: 0.3665 - val_loss: 0.3167\n",
            "Epoch 8/10\n",
            " - 5s - loss: 0.3658 - val_loss: 0.3175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c4e71c4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "um80RDbdmDz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bayesian Neural network\n",
        "\n",
        "TODO: fit the bayesian nnet w/ a single std dev param"
      ]
    },
    {
      "metadata": {
        "id": "h-L-4g268e03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "batch_size = 512\n",
        "max_steps = 2000\n",
        "learning_rate = 0.005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3oWEwP-8lye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DEBUGGER: reset the graph so you can re-run\n",
        "tf.reset_default_graph() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UKFXneM-_qF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the data into a training set and a validation set"
      ]
    },
    {
      "metadata": {
        "id": "65Ov_Whg-_KI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split data randomly into training + validation\n",
        "tr_ind = np.random.choice([False, True],\n",
        "                          size=x_taxi.shape[0])\n",
        "x_train = x_taxi[tr_ind]\n",
        "y_train = y_taxi[tr_ind]\n",
        "x_val = x_taxi[~tr_ind]\n",
        "y_val = y_taxi[~tr_ind]\n",
        "N_train = x_train.shape[0]\n",
        "N_val = x_val.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffWFkYHQ-6KD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build an input data pipeline"
      ]
    },
    {
      "metadata": {
        "id": "lWLy7zx88nDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_input_pipeline(x, y, x_val, y_val, batch_size, N_val):\n",
        "  '''Build an Iterator switching between train and heldout data.\n",
        "  Args:\n",
        "    x: Numpy `array` of training features, indexed by the first dimension.\n",
        "    y: Numpy `array` of training labels, with the same first dimension as `x`.\n",
        "    x_val: Numpy `array` of validation features, indexed by the first dimension.\n",
        "    y_val: Numpy `array` of validation labels, with the same first dimension as `x_val`.\n",
        "    batch_size: Number of elements in each training batch.\n",
        "    N_val: Number of examples in the validation dataset\n",
        "  Returns:\n",
        "    batch_features: `Tensor` feed  features, of shape\n",
        "      `[batch_size] + x.shape[1:]`.\n",
        "    batch_labels: `Tensor` feed of labels, of shape\n",
        "      `[batch_size] + y.shape[1:]`.\n",
        "  '''\n",
        "  # Build an iterator over training batches.\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  training_batches = training_dataset.shuffle(\n",
        "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
        "  train_iterator = training_batches.make_one_shot_iterator()\n",
        "\n",
        "  # Build a iterator over the validation set with batch_size=N_val,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "  val_frozen = val_dataset.take(N_val).repeat().batch(N_val)\n",
        "  val_iterator = val_frozen.make_one_shot_iterator()\n",
        "\n",
        "  # Combine these into a feedable iterator that can switch between training\n",
        "  # and validation inputs.\n",
        "  handle = tf.placeholder(tf.string, shape=[])\n",
        "  feedable_iterator = tf.data.Iterator.from_string_handle(\n",
        "      handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  batch_features, batch_labels = feedable_iterator.get_next()\n",
        "\n",
        "  return batch_features, batch_labels, handle, train_iterator, val_iterator\n",
        "\n",
        "# Build input pipeline\n",
        "x_vals, y_vals,  handle, training_iterator, validation_iterator = (\n",
        "    build_input_pipeline(x_train, y_train, \n",
        "                         x_val, y_val, \n",
        "                         batch_size, N_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3b_dBGIQ-Boh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to make a variationally-inferred parameter...."
      ]
    },
    {
      "metadata": {
        "id": "rLmMhVbJ-Ar3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def VariationalParameter(name, shape, constraint=None):\n",
        "  \"\"\"Generates variational distribution(s)\"\"\"\n",
        "  means = tf.get_variable(name+'_mean', \n",
        "                          initializer=tf.ones([1]),\n",
        "                          constraint=constraint)\n",
        "  stds = tf.get_variable(name+'_std', \n",
        "                         initializer=-2.3*tf.ones([1]))\n",
        "  return tfd.Normal(loc=means, scale=tf.math.exp(stds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqfqmlAf-RWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And a function to generate a densely-connected bayesian neural network with 1 continuous output."
      ]
    },
    {
      "metadata": {
        "id": "UqTcE5sZ8sC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A dense network specified by a list of Nunits per layer\n",
        "def DenseNetReg(units_per_layer, x_in)\n",
        "  prior = tfp.layers.default_mean_field_normal_fn()\n",
        "  layers = []\n",
        "  for units in units_per_layer:\n",
        "    layer = tfp.layers.DenseFlipout(\n",
        "      units=units, \n",
        "      kernel_posterior_fn=prior,\n",
        "      bias_posterior_fn=prior)\n",
        "    x_in = layer(x_in)\n",
        "    layers.append(layer)\n",
        "  layer = tfp.layers.DenseFlipout( #final layer w/ no activation\n",
        "      units=1, \n",
        "      activation=None,\n",
        "      kernel_posterior_fn=prior,\n",
        "      bias_posterior_fn=prior)\n",
        "  layer = layer(x_in)\n",
        "  layers.append(layer)\n",
        "  return layers, layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eu1K515k-a39",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And then we can build a model where the prediction is estimated by the bayesian neural network, and there is also an error term (which remains constant over the independent variable)"
      ]
    },
    {
      "metadata": {
        "id": "0nDNlMQq-aUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A linear regression model: a Normal distribution\n",
        "# parameterized by location from a dense multi-layer net,\n",
        "# and std dev parameter\n",
        "with tf.name_scope(\"bayesian_regression\", values=[x_vals]):\n",
        "  layers, predictions = DenseNetReg([128, 64], x_vals)\n",
        "  constraint = tf.keras.constraints.NonNeg()\n",
        "  noise_std = VariationalParameter('noise_std', [1],\n",
        "                                   constraint=constraint)\n",
        "  pred_distribution = tfd.Normal(loc=predictions, \n",
        "                                 scale=noise_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WL6ExZT6Aqm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To train the model , we'll use the evidence lower bound (ELBO) loss for variational inference, mean absolute error as a metric for evaluation, and the Adam optimizer."
      ]
    },
    {
      "metadata": {
        "id": "JcAa-o8kAqIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute the -ELBO as the loss, averaged over the batch size\n",
        "neg_log_likelihood = -tf.reduce_mean(pred_distribution.log_prob(y_vals))\n",
        "kl_div = sum([sum(layer.losses) for layer in layers]) / N\n",
        "elbo_loss = neg_log_likelihood + kl_div\n",
        "\n",
        "# Mean absolute error metric for evaluation\n",
        "mae, mae_update_op = tf.metrics.mean_absolute_error(\n",
        "    labels=y_vals, predictions=predictions)\n",
        "\n",
        "# Use ADAM optimizer w/ -ELBO loss\n",
        "with tf.name_scope(\"train\"):\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(elbo_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9xPrOhpBZPa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now we can train the model while recording the weights over training."
      ]
    },
    {
      "metadata": {
        "id": "tuf9qGdqBYcT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To store parameter values over training\n",
        "#weight_mean = layer.kernel_posterior.mean()[:,0]\n",
        "#weight_std = layer.kernel_posterior.stddev()[:,0]\n",
        "#bias_mean = layer.bias_posterior.mean()\n",
        "#bias_std = layer.bias_posterior.stddev()\n",
        "noise_std_mean = noise_std.mean()\n",
        "noise_std_std = noise_std.stddev()\n",
        "#weight_means = np.zeros((max_steps, D))\n",
        "#weight_stds = np.zeros((max_steps, D))\n",
        "#bias_means = np.zeros(max_steps)\n",
        "#bias_stds = np.zeros(max_steps)\n",
        "noise_stds = np.zeros(max_steps)\n",
        "noise_means = np.zeros(max_steps)\n",
        "maes = np.zeros(max_steps)\n",
        "losses = np.zeros(max_steps)\n",
        "\n",
        "# Initialization op\n",
        "init_op = tf.group(tf.global_variables_initializer(),\n",
        "                   tf.local_variables_initializer())\n",
        "\n",
        "# Run the training session\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "\n",
        "  # Training loop\n",
        "  train_handle = sess.run(training_iterator.string_handle())\n",
        "  val_handle = sess.run(validation_iterator.string_handle())\n",
        "  for iS in range(max_steps):\n",
        "    [\n",
        "        _,\n",
        "        _,\n",
        "        maes[iS],\n",
        "        losses[iS],\n",
        "        #weight_means[iS,:],\n",
        "        #weight_stds[iS,:],\n",
        "        #bias_means[iS],\n",
        "        #bias_stds[iS],\n",
        "        noise_means[iS],\n",
        "        noise_stds[iS]\n",
        "    ] = sess.run([\n",
        "        train_op,\n",
        "        mae_update_op,\n",
        "        mae,\n",
        "        elbo_loss,\n",
        "        #weight_mean,\n",
        "        #weight_std,\n",
        "        #bias_mean,\n",
        "        #bias_std,\n",
        "        noise_std_mean,\n",
        "        noise_std_std\n",
        "    ], feed_dict={handle: train_handle})\n",
        "\n",
        "  # Draw samples from the posterior\n",
        "  #Nmc = 1000\n",
        "  #w_draw = layer.kernel_posterior.sample(Nmc)\n",
        "  #b_draw = layer.bias_posterior.sample(Nmc)\n",
        "  #n_draw = noise_std.sample(Nmc)\n",
        "  #w_post, b_post, n_post = sess.run([w_draw, b_draw, n_draw])\n",
        "  \n",
        "  # Draw predictive distribution samples\n",
        "  Nmc = 1000\n",
        "  prediction_dists = sess.run((pred_distribution.sample(Nmc)), \n",
        "                              feed_dict={handle: val_handle})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6LHjuVpEpMZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: look at stuff over the course of training"
      ]
    },
    {
      "metadata": {
        "id": "Oi75jvdhElUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot mean absoolute error over training\n",
        "plt.figure()\n",
        "plt.plot(maes[1:])\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.title('Mean Absolute Error over training')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsKLogeEExAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot ELBO loss over training\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('ELBO Loss')\n",
        "plt.title('ELBO loss over training')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwBMSb9wE6qi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot value of noise std dev over training\n",
        "plt.figure()\n",
        "plt.plot(noise_means, label='fit')\n",
        "plt.axhline(noise_std_true, label='true', linestyle='--')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Noise std dev mean')\n",
        "plt.title('Noise Std Dev parameter over training')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4rjxqdiE8VE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot value of bias std dev over training\n",
        "plt.figure()\n",
        "plt.plot(noise_stds, label='fit')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Noise std dev scale')\n",
        "plt.title('Noise Std Dev scale parameter over training')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyhnXw_eCoGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: look at predictive distributions"
      ]
    },
    {
      "metadata": {
        "id": "a9Zl1Hz2Flql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot random datapoints and their prediction distributions\n",
        "fig, axes = plt.subplots(4, 2, sharex='all')\n",
        "for i in range(4):\n",
        "  for j in range(2):\n",
        "    ix = i*2+j\n",
        "    sns.kdeplot(prediction_dists[:,ix,0], \n",
        "                shade=True, ax=axes[i][j])\n",
        "    axes[i][j].axvline(x=y_val[ix,0])\n",
        "plt.gcf().suptitle('Predictive Distributions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fdFNGnLEUAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: compute coverage of the 95% interval"
      ]
    },
    {
      "metadata": {
        "id": "YmSMr9DjGRcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def covered(dist, true, prc=95.0):\n",
        "  q0 = (1.0-prc)/2.0 #lower percentile \n",
        "  q1 = 1.0-q0        #upper percentile\n",
        "  within_conf_int = np.zeros(len(true))\n",
        "  for i in range(len(true)):\n",
        "    p0 = np.percentile(dist[:,i], q0)\n",
        "    p1 = np.percentile(dist[:,i], q1)\n",
        "    if p0<=true[i] and p1>true[i]:\n",
        "      within_conf_int[i] = 1\n",
        "  return within_conf_int\n",
        "\n",
        "def coverage(dist, true, prc=95.0):\n",
        "  return np.mean(covered(dist, true, prc))\n",
        "\n",
        "print('Coverage of 95%% predictive interval: %0.1f%%'\n",
        "      % coverage(prediction_dists[:,:,0], y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOYpIR3AEW7A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: talk about how b/c we assumed a constant error term, the model doesn't handle the heterosskedastic nature of the data very well\n",
        "\n",
        "TODO: look at coverage across time"
      ]
    },
    {
      "metadata": {
        "id": "61OIthwuFfso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot coverage as a fn of time of day\n",
        "covs = np.zeros(24)\n",
        "for iT in range(0,24):\n",
        "  ix = ((x_val['min_of_day']>=(iT*60)) &\n",
        "        (x_val['min_of_day']<((iT+1)*60)))\n",
        "  covs[iT] = coverage(prediction_dists[:,ix,0], y_val[ix])\n",
        "plt.plot(covs, label='True')\n",
        "plt.axhline(95.0, label='Ideal', color=colors[2])\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('95% Predictive Interval Coverage')\n",
        "plt.title('Coverage of 95% Predictive Interval by Hour')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpLWGF4SKTJO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and we can also look at the coverage across pickup location"
      ]
    },
    {
      "metadata": {
        "id": "Pa9Ft4zXImHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create vaex df with predictive intervals\n",
        "cov_by_loc = pd.DataFrame()\n",
        "cov_by_loc['x'] = x_val['pickup_longitude']\n",
        "cov_by_loc['y'] = x_val['pickup_latitude']\n",
        "cov_by_loc['cov'] = covered(prediction_dists[:,:,0], y_val)\n",
        "vdf = vaex.from_pandas(cov_by_loc)\n",
        "\n",
        "# Compute coverage of the predictive interval\n",
        "lims = [[-74.1, -73.75],[40.6, 40.9]]\n",
        "cov = vdf.mean(vdf.cov, limits=lims, shape=300,\n",
        "               binby=[vdf.pickup_longitude,\n",
        "                      vdf.pickup_latitude])\n",
        "\n",
        "# Plot coverage of the predictive interval\n",
        "cmap = matplotlib.cm.PuOr\n",
        "cmap.set_bad('black', 1.)\n",
        "plt.imshow(cov.T, origin='lower',\n",
        "           vmin=0.9, vmax=1.0, cmap=cmap,\n",
        "           extent=[lims[0][0], lims[0][1], \n",
        "                   lims[1][0], lims[1][1]])\n",
        "ax = plt.gca()\n",
        "ax.grid(False)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Coverage of 95% predictive interval', \n",
        "               rotation=270)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Duj75IByKX3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: not great! this is because our model assumes the error is constant across our independent variables.  To build a model which allows the error to vary across the predictors, we can build a model which predicts the uncertainty separately from the mean!"
      ]
    },
    {
      "metadata": {
        "id": "bBYacilqmLtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dual-Module Bayesian Neural Network\n",
        "\n",
        "TODO: fit the bayesian nnet which separately estimates the unceratinty from the prediction\n",
        "\n",
        "compare coverage of 95% interval and performance w/ heteroskedasticity to the first nnet"
      ]
    },
    {
      "metadata": {
        "id": "A5mxi1Aqyopv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DEBUGGER: reset the graph so you can re-run\n",
        "tf.reset_default_graph() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E62qD8SdyqPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build fresh input pipeline\n",
        "x_vals, y_vals,  handle, training_iterator, validation_iterator = (\n",
        "    build_input_pipeline(x_train, y_train, \n",
        "                         x_val, y_val, \n",
        "                         batch_size, N_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXeF_UPBytlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A linear regression model: a Normal distribution\n",
        "# parameterized by location from a dense multi-layer net,\n",
        "# and std dev from a separate dense multi-layer net\n",
        "with tf.name_scope(\"dual_module_regression\", values=[x_vals]):\n",
        "  pred_layers, predictions = DenseNetReg([32, 16, 8], x_vals)  \n",
        "  noise_layers, noise_std = DenseNetReg([32, 16, 8], x_vals)  \n",
        "  pred_distribution = tfd.Normal(loc=predictions, \n",
        "                                 scale=noise_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Lv1-GWxzCkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute the -ELBO as the loss, averaged over the batch size\n",
        "neg_log_likelihood = -tf.reduce_mean(pred_distribution.log_prob(y_vals))\n",
        "kl_div = (sum([sum(pred_layers.losses) for layer in pred_layer]) +\n",
        "          sum([sum(noise_layers.losses) for layer in pred_layer])) / N\n",
        "elbo_loss = neg_log_likelihood + kl_div\n",
        "\n",
        "# Mean absolute error metric for evaluation\n",
        "mae, mae_update_op = tf.metrics.mean_absolute_error(\n",
        "    labels=y_vals, predictions=predictions)\n",
        "\n",
        "# Use ADAM optimizer w/ -ELBO loss\n",
        "with tf.name_scope(\"train\"):\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(elbo_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zMh7NDC8zEFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To store parameter values over training\n",
        "#weight_mean = layer.kernel_posterior.mean()[:,0]\n",
        "#weight_std = layer.kernel_posterior.stddev()[:,0]\n",
        "#bias_mean = layer.bias_posterior.mean()\n",
        "#bias_std = layer.bias_posterior.stddev()\n",
        "noise_std_mean = noise_std.mean()\n",
        "noise_std_std = noise_std.stddev()\n",
        "#weight_means = np.zeros((max_steps, D))\n",
        "#weight_stds = np.zeros((max_steps, D))\n",
        "#bias_means = np.zeros(max_steps)\n",
        "#bias_stds = np.zeros(max_steps)\n",
        "noise_stds = np.zeros(max_steps)\n",
        "noise_means = np.zeros(max_steps)\n",
        "maes = np.zeros(max_steps)\n",
        "losses = np.zeros(max_steps)\n",
        "\n",
        "# Initialization op\n",
        "init_op = tf.group(tf.global_variables_initializer(),\n",
        "                   tf.local_variables_initializer())\n",
        "\n",
        "# Run the training session\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "\n",
        "  # Training loop\n",
        "  train_handle = sess.run(training_iterator.string_handle())\n",
        "  val_handle = sess.run(validation_iterator.string_handle())\n",
        "  for iS in range(max_steps):\n",
        "    [\n",
        "        _,\n",
        "        _,\n",
        "        maes[iS],\n",
        "        losses[iS],\n",
        "        #weight_means[iS,:],\n",
        "        #weight_stds[iS,:],\n",
        "        #bias_means[iS],\n",
        "        #bias_stds[iS],\n",
        "        noise_means[iS],\n",
        "        noise_stds[iS]\n",
        "    ] = sess.run([\n",
        "        train_op,\n",
        "        mae_update_op,\n",
        "        mae,\n",
        "        elbo_loss,\n",
        "        #weight_mean,\n",
        "        #weight_std,\n",
        "        #bias_mean,\n",
        "        #bias_std,\n",
        "        noise_std_mean,\n",
        "        noise_std_std\n",
        "    ], feed_dict={handle: train_handle})\n",
        "\n",
        "  # Draw samples from the posterior\n",
        "  #Nmc = 1000\n",
        "  #w_draw = layer.kernel_posterior.sample(Nmc)\n",
        "  #b_draw = layer.bias_posterior.sample(Nmc)\n",
        "  #n_draw = noise_std.sample(Nmc)\n",
        "  #w_post, b_post, n_post = sess.run([w_draw, b_draw, n_draw])\n",
        "  \n",
        "  # Draw predictive distribution samples\n",
        "  Nmc = 1000\n",
        "  prediction_dists_dual = sess.run((pred_distribution.sample(Nmc)), \n",
        "                                   feed_dict={handle: val_handle})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K9gpE5LczLpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot mean absoolute error over training\n",
        "plt.figure()\n",
        "plt.plot(maes[1:])\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.title('Mean Absolute Error over training')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFaszD0CMP_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot ELBO loss over training\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('ELBO Loss')\n",
        "plt.title('ELBO loss over training')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBL2puuMMTpO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot value of noise std dev over training\n",
        "plt.figure()\n",
        "plt.plot(noise_means, label='fit')\n",
        "plt.axhline(noise_std_true, label='true', linestyle='--')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Noise std dev mean')\n",
        "plt.title('Noise Std Dev parameter over training')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCSaqZCfMaKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot value of bias std dev over training\n",
        "plt.figure()\n",
        "plt.plot(noise_stds, label='fit')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Noise std dev scale')\n",
        "plt.title('Noise Std Dev scale parameter over training')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8eqYjiuMc1K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and we can look at some predictive distributions\n",
        "\n",
        "TODO: compare to the predictive distributions of the model w/ constant error"
      ]
    },
    {
      "metadata": {
        "id": "sd_Dm4UXMj4M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot random datapoints and their prediction distributions\n",
        "fig, axes = plt.subplots(4, 2, sharex='all')\n",
        "for i in range(4):\n",
        "  for j in range(2):\n",
        "    ix = i*2+j\n",
        "    sns.kdeplot(prediction_dists_dual[:,ix,0], \n",
        "                shade=True, ax=axes[i][j])\n",
        "    axes[i][j].axvline(x=y_val[ix,0])\n",
        "plt.gcf().suptitle('Predictive Distributions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZeD_mu9Mqbp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: comparing the coverage of the 95% interval for the model w/ constant error vs the dual-module net"
      ]
    },
    {
      "metadata": {
        "id": "WAopAOHsMp0C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compare coverages between models\n",
        "print('Coverage of 95%% interval for model w /constant error: %0.1f%%'\n",
        "      % coverage(prediction_dists[:,:,0], y_val))\n",
        "print('Coverage of 95%% interval for dual-module model: %0.1f%%'\n",
        "      % coverage(prediction_dists_dual[:,:,0], y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFpAbQ0gNGsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and comparing the coverage between models as fn of time of day"
      ]
    },
    {
      "metadata": {
        "id": "QXNY1iorNJb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot coverage as a fn of time of day\n",
        "covs = np.zeros(24)\n",
        "covs_dual = np.zeros(24)\n",
        "for iT in range(0,24):\n",
        "  ix = ((x_val['min_of_day']>=(iT*60)) &\n",
        "        (x_val['min_of_day']<((iT+1)*60)))\n",
        "  covs[iT] = coverage(prediction_dists[:,ix,0], y_val[ix])\n",
        "  covs_dual[iT] = coverage(prediction_dists_dual[:,ix,0], y_val[ix])\n",
        "plt.plot(covs, label='Single')\n",
        "plt.plot(covs_dual, label='Dual')\n",
        "plt.axhline(95.0, label='Ideal', color=colors[2])\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('95% Predictive Interval Coverage')\n",
        "plt.title('Coverage of 95% Predictive Interval by Hour')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kz2EpfkHNeP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and comparing coverage of interval across location"
      ]
    },
    {
      "metadata": {
        "id": "KZvnDnkQNiMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create vaex df with predictive intervals\n",
        "cov_by_loc = pd.DataFrame()\n",
        "cov_by_loc['x'] = x_val['pickup_longitude']\n",
        "cov_by_loc['y'] = x_val['pickup_latitude']\n",
        "cov_by_loc['cov'] = covered(prediction_dists[:,:,0], y_val)\n",
        "cov_by_loc['cov_dual'] = covered(prediction_dists_dual[:,:,0], y_val)\n",
        "vdf = vaex.from_pandas(cov_by_loc)\n",
        "\n",
        "# Compute coverage of the predictive interval\n",
        "lims = [[-74.1, -73.75],[40.6, 40.9]]\n",
        "cov = vdf.mean(vdf.cov, limits=lims, shape=300,\n",
        "               binby=[vdf.pickup_longitude,\n",
        "                      vdf.pickup_latitude])\n",
        "cov_dual = vdf.mean(vdf.cov_dual, \n",
        "                    limits=lims, shape=300,\n",
        "                    binby=[vdf.pickup_longitude,\n",
        "                           vdf.pickup_latitude])\n",
        "\n",
        "# Plot coverage of the predictive interval\n",
        "cmap = matplotlib.cm.PuOr\n",
        "cmap.set_bad('black', 1.)\n",
        "plt.imshow(cov.T, origin='lower',\n",
        "           vmin=0.9, vmax=1.0, cmap=cmap,\n",
        "           extent=[lims[0][0], lims[0][1], \n",
        "                   lims[1][0], lims[1][1]])\n",
        "ax = plt.gca()\n",
        "ax.grid(False)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Coverage of 95% predictive interval', \n",
        "               rotation=270)\n",
        "plt.title('Coverage of 95% Predictive Interval for Model w/ Constant Error')\n",
        "plt.show()\n",
        "\n",
        "# Plot coverage of the predictive interval\n",
        "plt.imshow(cov_dual.T, origin='lower',\n",
        "           vmin=0.9, vmax=1.0, cmap=cmap,\n",
        "           extent=[lims[0][0], lims[0][1], \n",
        "                   lims[1][0], lims[1][1]])\n",
        "ax = plt.gca()\n",
        "ax.grid(False)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Coverage of 95% predictive interval', \n",
        "               rotation=270)\n",
        "plt.title('Coverage of 95% Predictive Interval for Dual-Module Model')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6mldoLcR3Ar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: looks like the dual-module network handles heteroskedasticity better, but we can plot which is closer to 95% as a function of location\n",
        "\n",
        "TODO: use a 2d colormap here? goes from red to blue depending on which model is better, but from bright to black depending on how many datapoints there are @ that pixel\n",
        "\n",
        "Like here: https://github.com/JohannesBuchner/uncertaincolors\n",
        "\n",
        "Could just do it manually (compute the color for each pixel, that is)\n",
        "\n",
        "Maybe via a lambda func like this: https://stackoverflow.com/questions/41966600/matplotlib-colormap-with-two-parameter\n",
        "\n",
        "And just use Blue (0,0,1) to red (1,0,0) for the colormap w/ brightness of the number of observations in that pixel (so multiply the color value by 0 if num obs=0 and by 1 if num obs = max num obs)"
      ]
    },
    {
      "metadata": {
        "id": "0vJJEKRbR-Fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot difference in deviation from 95% coverage\n",
        "diff = np.abs(cov_dual-0.95) - np.abs(cov-0.95)\n",
        "cmax = np.abs(diff).max()\n",
        "plt.imshow(cov_dual.T, \n",
        "           origin='lower', cmap=cmap,\n",
        "           vmin=-cmax, vmax=cmax, \n",
        "           extent=[lims[0][0], lims[0][1], \n",
        "                   lims[1][0], lims[1][1]])\n",
        "ax = plt.gca()\n",
        "ax.grid(False)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Dual-module Model is Better <-- --> Constant Eror Model is Better', \n",
        "               rotation=270)\n",
        "plt.title('Which model\\'s uncertainty estimate is better across location')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YEKSTD1TYIl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and zooming in on Jersey city and Hoboken, we can see that the dual module net is far better at predicting the uncertainty there than the constant-error net"
      ]
    },
    {
      "metadata": {
        "id": "ph-7F9KUTl-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute coverage of the predictive interval\n",
        "lims = [[-74.07, -73.96],[40.70, 40.77]]\n",
        "cov = vdf.mean(vdf.cov, limits=lims, shape=300,\n",
        "               binby=[vdf.pickup_longitude,\n",
        "                      vdf.pickup_latitude])\n",
        "cov_dual = vdf.mean(vdf.cov_dual, \n",
        "                    limits=lims, shape=300,\n",
        "                    binby=[vdf.pickup_longitude,\n",
        "                           vdf.pickup_latitude])\n",
        "\n",
        "# TODO: plot of abs(cov_dual-0.95)-abs(cov-0.95)\n",
        "# Plot difference in deviation from 95% coverage\n",
        "diff = np.abs(cov_dual-0.95) - np.abs(cov-0.95)\n",
        "cmax = np.abs(diff).max()\n",
        "plt.imshow(cov_dual.T, \n",
        "           origin='lower', cmap=cmap,\n",
        "           vmin=-cmax, vmax=cmax, \n",
        "           extent=[lims[0][0], lims[0][1], \n",
        "                   lims[1][0], lims[1][1]])\n",
        "ax = plt.gca()\n",
        "ax.grid(False)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Dual-module Model is Better <-- --> Constant Eror Model is Better', \n",
        "               rotation=270)\n",
        "plt.title('Which model\\'s uncertainty estimate is better across location')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENfg-V8RN-Ph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: talk about how the dual-module net handles heteroskedasticity much better!"
      ]
    }
  ]
}